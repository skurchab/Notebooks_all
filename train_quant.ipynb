{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.special import erf\n",
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training set\n",
    "d1=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150301.csv\")\n",
    "d2=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150302.csv\")\n",
    "d3=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150303.csv\")\n",
    "d4=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150304.csv\")\n",
    "d5=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150305.csv\")\n",
    "d6=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150306.csv\")\n",
    "d7=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150307.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d=[d1,d2,d3,d4,d5,d5,d6,d7]\n",
    "df=pd.concat(d, ignore_index=True)\n",
    "df_f1=df[df.TX_FRAUD==1]\n",
    "df_f0=df[df.TX_FRAUD==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_f0_s=df_f0.sample(6124)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sl=[df_f0_s,df_f1]\n",
    "df_s=pd.concat(df_sl,ignore_index=True)\n",
    "df_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fr=df_s['TX_FRAUD'].apply(str)\n",
    "age=df_s['AGE']\n",
    "am=df_s['TX_AMOUNT']\n",
    "ID=df_s['CARD_PAN_ID']\n",
    "last=df_s['NB_TRX_LAST_24H']\n",
    "time_s=df_s['TX_TIME_SECONDS']\n",
    "new=[ID,am,age,fr,time_s,last]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training=pd.concat([ID, am, age, fr, time_s, last], axis=1)\n",
    "training.to_csv(\"select/training\", index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150314.csv\")\n",
    "testt=pd.concat([t.CARD_PAN_ID,t.TX_AMOUNT,t.AGE,t.TX_FRAUD.apply(str),t.TX_TIME_SECONDS,t.NB_TRX_LAST_24H],axis=1)\n",
    "testt.to_csv(\"select/for_test\",index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t[t.TX_FRAUD==1].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names=['card_pan_id','tx_amount','age','tx_time_seconds','nb_tx_last_24h']\n",
    "response_col=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train = h2o.import_file(\"select/training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_val=h2o.import_file(\"select/for_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_col=['CARD_PAN_ID','TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "resp_col='TX_FRAUD'\n",
    "type('TX_FRAUD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "model = H2ORandomForestEstimator(ntrees=50, max_depth=20, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.train(x=train_col, y=resp_col, training_frame=df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred=model.predict(df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "pr.info()\n",
    "tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "tr.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sklearn.metrics.roc_curve(y_true, y_score, pos_label=None, sample_weight=None, drop_intermediate=True)\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "roc_auc = auc(false_positive_rate, true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title('ROC_test')\n",
    "plt.plot(false_positive_rate, true_positive_rate, 'r',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'g--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "new=pd.concat(n,axis=1)\n",
    "gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "top=gr.head(100)\n",
    "per=(top[top.TX_FRAUD==1].count())/100\n",
    "perc=per.TX_FRAUD # frauds in top 100\n",
    "frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "top[top.TX_FRAUD==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sklearn.metrics.precision_recall_curve(y_true, probas_pred, pos_label=None, sample_weight=None)\n",
    "precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "#sklearn.metrics.average_precision_score(y_true, y_score, average='macro', sample_weight=None)\n",
    "average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "plt.clf()\n",
    "plt.plot(recall, precision, color='navy',\n",
    "         label='Precision-Recall')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('Precision-Recall example: AUC={0:0.2f}'.format(average_precision))\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sales = [{'account': 'Jones LLC', 'Jan': 150, 'Feb': 200, 'Mar': 140},\n",
    "         {'account': 'Alpha Co',  'Jan': 200, 'Feb': 210, 'Mar': 215},\n",
    "         {'account': 'Blue Inc',  'Jan': 50,  'Feb': 90,  'Mar': 95 },\n",
    "         {'account': 'Jones LLC', 'Jan': 120, 'Feb': 220, 'Mar': 240},\n",
    "         {'account': 'Blue Inc',  'Jan': 60,  'Feb': 80,  'Mar': 105 }]\n",
    "frame = pd.DataFrame(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k=[1,2,3,4,5,6,7,7,8,9]\n",
    "for i in k:\n",
    "    i=i+1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in test:\n",
    "    pd.concat([i.CARD_PAN_ID,i.TX_AMOUNT,i.AGE,i.TX_FRAUD.apply(str),i.TX_TIME_SECONDS,i.NB_TRX_LAST_24H],axis=1)\n",
    "    tocsv(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tocsv(n):\n",
    "    n.to_csv(\"select/*\",index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reading(files):\n",
    "    frame_test=[]\n",
    "    for f in files:\n",
    "        df=pd.read_csv(f)\n",
    "        frame_test.append(df)\n",
    "        del df\n",
    "    df_final=pd.concat(frame_test,)\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fill=filter(fil, [join(dir_input, f) for f in listdir(dir_input) if isfile(join(dir_input, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reading(fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = r'/home/worldline/data/CSVagg'\n",
    "filenames =sorted(glob(path + \"/*.csv\"))\n",
    "filenames_te1=filenames[filenames.index(path+\"/aggtrx_20150314.csv\"):(filenames.index(path+\"/aggtrx_20150321.csv\")+1)]\n",
    "filenames_te2=filenames[filenames.index(path+\"/aggtrx_20150322.csv\"):(filenames.index(path+\"/aggtrx_20150329.csv\")+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#################################\n",
    "#preparation data for the testing\n",
    "for f in filenames_te:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#preparation data for training\n",
    "#filenames_tr=filenames[filenames.index(path+\"/aggtrx_20150301.csv\"):(filenames.index(path+\"/aggtrx_20150307.csv\")+1)]\n",
    "#d=[]\n",
    "#for k in filenames_tr:\n",
    "    #data_p=pd.read_csv(k)\n",
    "    #data=pd.concat([data_p.CARD_PAN_ID,data_p.TX_AMOUNT,data_p.AGE,data_p.TX_FRAUD.apply(str),data_p.TX_TIME_SECONDS,data_p.NB_TRX_LAST_24H],axis=1)\n",
    "    #d=data\n",
    "    #df=pd.concat(d, ignore_index=True)\n",
    "    #print(d)\n",
    "#df=pd.concat(d, ignore_index=True)\n",
    "#df_f1=df[df.TX_FRAUD==1]\n",
    "#df_f0=df[df.TX_FRAUD==0]\n",
    "#df_f0_s=df_f0.sample(6124)\n",
    "#df_sl=[df_f0_s,df_f1]\n",
    "#df_s=pd.concat(df_sl,ignore_index=True)\n",
    "#data=pd.concat([df_s.CARD_PAN_ID,df_s.TX_AMOUNT,df_s.AGE,df_s.TX_FRAUD.apply(str),df_s.TX_TIME_SECONDS,df_s.NB_TRX_LAST_24H],axis=1)\n",
    "#specname = k.split(\"/\")[-1]\n",
    "#data.to_csv(os.path.join('select/train'))\n",
    "#data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####################################\n",
    "new_path=r'select'\n",
    "names =sorted(glob(new_path + \"/*.csv\"))\n",
    "type(names)\n",
    "names[names.index(new_path+\"/aggtrx_20150301.csv\")]\n",
    "names[names.index(new_path+\"/aggtrx_20150307.csv\")]\n",
    "training=names[names.index(new_path+\"/aggtrx_20150301.csv\"):(names.index(new_path+\"/aggtrx_20150307.csv\")+1)]\n",
    "test=names[names.index(new_path+\"/aggtrx_20150314.csv\"):(names.index(new_path+\"/aggtrx_20150530.csv\")+1)]\n",
    "#for i in training:\n",
    "    #df_train = h2o.import_file('select/%s'% specname)\n",
    "   \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#testting\n",
    "path = r'/home/worldline/data/CSVagg'\n",
    "filenames =sorted(glob(path + \"/*.csv\"))\n",
    "filenames_te1=filenames[filenames.index(path+\"/aggtrx_20150314.csv\"):(filenames.index(path+\"/aggtrx_20150321.csv\")+1)]\n",
    "filenames_te2=filenames[filenames.index(path+\"/aggtrx_20150322.csv\"):(filenames.index(path+\"/aggtrx_20150329.csv\")+1)]\n",
    "filenames_te3=filenames[filenames.index(path+\"/aggtrx_20150330.csv\"):(filenames.index(path+\"/aggtrx_20150406.csv\")+1)]\n",
    "filenames_te4=filenames[filenames.index(path+\"/aggtrx_20150407.csv\"):(filenames.index(path+\"/aggtrx_20150414.csv\")+1)]\n",
    "filenames_te5=filenames[filenames.index(path+\"/aggtrx_20150415.csv\"):(filenames.index(path+\"/aggtrx_20150422.csv\")+1)]\n",
    "filenames_te6=filenames[filenames.index(path+\"/aggtrx_20150423.csv\"):(filenames.index(path+\"/aggtrx_20150430.csv\")+1)]\n",
    "filenames_te7=filenames[filenames.index(path+\"/aggtrx_20150501.csv\"):(filenames.index(path+\"/aggtrx_20150508.csv\")+1)]\n",
    "filenames_te8=filenames[filenames.index(path+\"/aggtrx_20150509.csv\"):(filenames.index(path+\"/aggtrx_20150516.csv\")+1)]\n",
    "filenames_te9=filenames[filenames.index(path+\"/aggtrx_20150516.csv\"):(filenames.index(path+\"/aggtrx_20150523.csv\")+1)]\n",
    "filenames_te10=filenames[filenames.index(path+\"/aggtrx_20150524.csv\"):(filenames.index(path+\"/aggtrx_20150531.csv\")+1)]\n",
    "filenames_te=[filenames_te1,filenames_te2,filenames_te3,filenames_te4,filenames_te5,filenames_te6,filenames_te7,filenames_te8,filenames_te9,filenames_te10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Main all variables\n",
    "ROC_AUC=[]\n",
    "Average_Precision=[]\n",
    "Share=[]\n",
    "Fraud_ID=[]\n",
    "  h2o.init(port=54331)\n",
    "    print('The {} is done!'.format(file))\n",
    "    Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                              \"Average_Precision\": Average_Precision,\n",
    "                              \"Share\": Share})\n",
    "A_P_m=Final_table.Average_Precision.mean()\n",
    "AUC_m=Final_table.ROC_AUC.mean()\n",
    "share_m=Final_table.Share.mean()\n",
    "print(\"Average precision mean: {}, ROC AUC mean: {}, Share of fraudulant card in top 100 mean: {}\".format(A_P_m, AUC_m, share_m))\n",
    "Final_table\n",
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_P_m=Final_table.Average_Precision.mean()\n",
    "AUC_m=Final_table.ROC_AUC.mean()\n",
    "share_m=Final_table.Share.mean()\n",
    "print(\"Average precision mean: {}, ROC AUC mean: {}, Share of fraudulant card in top 100 mean: {}\".format(A_P_m, AUC_m, share_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Main\n",
    "ROC_AUC=[]\n",
    "Average_Precision=[]\n",
    "Share=[]\n",
    "Fraud_ID=[]\n",
    "h2o.init(port=54331)\n",
    "for f in filenames_te1:\n",
    "    h2o.cluster().shutdown()\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    h2o.init(port=54331)\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "    Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                              \"Average_Precision\": Average_Precision,\n",
    "                              \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "A_P_m=Final_table.Average_Precision.mean()\n",
    "AUC_m=Final_table.ROC_AUC.mean()\n",
    "share_m=Final_table.Share.mean()\n",
    "print(\"Average precision mean: {}, ROC AUC mean: {}, Share of fraudulant card in top 100 mean: {}\".format(A_P_m, AUC_m, share_m))\n",
    "Final_table\n",
    "h2o.cluster().shutdown()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te2:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(14)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te3:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(21)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te4:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(28)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te5:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(35)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te6:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te7:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(49)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te8:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(56)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te9:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(63)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h2o.init(port=54331)\n",
    "for f in filenames_te10:\n",
    "    data=pd.read_csv(f)\n",
    "    data=pd.concat([data.CARD_PAN_ID,data.TX_AMOUNT,data.AGE,data.TX_FRAUD.apply(str),data.TX_TIME_SECONDS,data.NB_TRX_LAST_24H],axis=1)\n",
    "    data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "    specname = f.split(\"/\")[-1]\n",
    "    data.to_csv(os.path.join('select/%s'% specname))\n",
    "    df_train = h2o.import_file(\"select/training\")\n",
    "    df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "    train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H']\n",
    "    resp_col='TX_FRAUD'\n",
    "    model = H2ORandomForestEstimator(ntrees=25, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "    model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "    pred=model.predict(df_val)\n",
    "    pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "    tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    ROC_AUC.append(roc_auc)\n",
    "    precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "    average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "    Average_Precision.append(average_precision)\n",
    "    n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "    new=pd.concat(n,axis=1)\n",
    "    gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "    top=gr.head(100)\n",
    "    per=(top[top.TX_FRAUD==1].count())/100\n",
    "    perc=per.TX_FRAUD # frauds in top 100\n",
    "    Share.append(perc)\n",
    "    frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "    Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    \n",
    "Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                          \"Average_Precision\": Average_Precision,\n",
    "                          \"Share\": Share})\n",
    "\n",
    "h2o.cluster().shutdown()\n",
    "Final_table.head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Final_table.ROC_AUC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(Final_table.Average_Precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A_P_m=Final_table.Average_Precision.mean()\n",
    "AUC_m=Final_table.ROC_AUC.mean()\n",
    "share_m=Final_table.Share.mean()\n",
    "print(A_P_m,AUC_m,share_m)\n",
    "print(\"Average precision mean: {}, ROC AUC mean: {}, Share of fraudulant card in top 100 mean: {}\".format(A_P_m, AUC_m, share_m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result=[]\n",
    "lis=[1,2,3,4,5,6,7,8,9,0]\n",
    "for i in lis:\n",
    "    k=i*2\n",
    "    result.append(k)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Final_table.ROC_AUC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
