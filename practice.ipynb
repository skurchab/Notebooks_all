{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.special import erf\n",
    "from scipy.stats import pearsonr, kendalltau, spearmanr\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150301.csv\")\n",
    "d2=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150302.csv\")\n",
    "d3=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150303.csv\")\n",
    "d4=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150304.csv\")\n",
    "d5=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150305.csv\")\n",
    "d6=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150306.csv\")\n",
    "d7=pd.read_csv(\"/home/worldline/data/CSVagg/aggtrx_20150307.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d=[d1,d2,d3,d4,d5,d5,d6,d7]\n",
    "df=pd.concat(d, ignore_index=True)\n",
    "df_f1=df[df.TX_FRAUD==1]\n",
    "df_f0=df[df.TX_FRAUD==0]\n",
    "df_f0_s=df_f0.sample(len(df[df.TX_FRAUD==1]))\n",
    "df_sl=[df_f0_s,df_f1]\n",
    "df_s=pd.concat(df_sl,ignore_index=True)\n",
    "del df_s['TX_ACCEPTED']\n",
    "df_s.to_csv(\"select/train_all\", index=False, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "len(df[df.TX_FRAUD==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import h2o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = r'/home/worldline/data/CSVagg'\n",
    "filenames =sorted(glob(path + \"/*.csv\"))\n",
    "#filenames_te=filenames[filenames.index(path+\"/aggtrx_20150314.csv\"):(filenames.index(path+\"/aggtrx_20150321.csv\")+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = r'/home/worldline/data/CSVagg'\n",
    "filenames =sorted(glob(path + \"/*.csv\"))\n",
    "filenames_te1=filenames[filenames.index(path+\"/aggtrx_20150314.csv\"):(filenames.index(path+\"/aggtrx_20150321.csv\")+1)]\n",
    "filenames_te2=filenames[filenames.index(path+\"/aggtrx_20150322.csv\"):(filenames.index(path+\"/aggtrx_20150329.csv\")+1)]\n",
    "filenames_te3=filenames[filenames.index(path+\"/aggtrx_20150330.csv\"):(filenames.index(path+\"/aggtrx_20150406.csv\")+1)]\n",
    "filenames_te4=filenames[filenames.index(path+\"/aggtrx_20150407.csv\"):(filenames.index(path+\"/aggtrx_20150414.csv\")+1)]\n",
    "filenames_te5=filenames[filenames.index(path+\"/aggtrx_20150415.csv\"):(filenames.index(path+\"/aggtrx_20150422.csv\")+1)]\n",
    "filenames_te6=filenames[filenames.index(path+\"/aggtrx_20150423.csv\"):(filenames.index(path+\"/aggtrx_20150430.csv\")+1)]\n",
    "filenames_te7=filenames[filenames.index(path+\"/aggtrx_20150501.csv\"):(filenames.index(path+\"/aggtrx_20150508.csv\")+1)]\n",
    "filenames_te8=filenames[filenames.index(path+\"/aggtrx_20150509.csv\"):(filenames.index(path+\"/aggtrx_20150516.csv\")+1)]\n",
    "filenames_te9=filenames[filenames.index(path+\"/aggtrx_20150516.csv\"):(filenames.index(path+\"/aggtrx_20150523.csv\")+1)]\n",
    "filenames_te10=filenames[filenames.index(path+\"/aggtrx_20150524.csv\"):(filenames.index(path+\"/aggtrx_20150531.csv\")+1)]\n",
    "filenames_te=[filenames_te1,filenames_te2,filenames_te3,filenames_te4,filenames_te5,filenames_te6,filenames_te7,filenames_te8,filenames_te9,filenames_te10]\n",
    "h2o.init(port=54331)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Main all variables\n",
    "ROC_AUC=[]\n",
    "Average_Precision=[]\n",
    "Share=[]\n",
    "Fraud_ID=[]\n",
    "for file in filenames_te:\n",
    "    for f in file:\n",
    "        h2o.cluster().shutdown()\n",
    "        data=pd.read_csv(f)\n",
    "        data.TX_FRAUD.apply(str)\n",
    "        del data['TX_ACCEPTED']\n",
    "        data=data[~ data.CARD_PAN_ID.isin(Fraud_ID)]\n",
    "        specname = f.split(\"/\")[-1]\n",
    "        data.to_csv(os.path.join('select/%s'% specname))\n",
    "        h2o.init(port=54331)\n",
    "        df_train = h2o.import_file(\"select/train_all\")\n",
    "        df_val=h2o.import_file(os.path.join('select/%s'% specname))\n",
    "        train_col=['TX_AMOUNT','AGE','TX_TIME_SECONDS','NB_TRX_LAST_24H','TERM_MIDUID','TERM_MCC','TERM_COUNTRY','TX_3D_SECURE',\n",
    "                  'LANGUAGE','GENDER','BROKER','CARD_BRAND','TX_TIME_HOURS','TX_TIME_DAYS','TERM_REGION','TERM_CONTINENT','TERM_MCCG','TERM_MCC_GROUP',\n",
    "                  'SUM_AMT_LAST_24H','TX_DIFF_LAST_TX','LAST_MIDUID_TX','LAST_COUNTRY_TX','LAST_MCC_HIS']\n",
    "        resp_col='TX_FRAUD'\n",
    "        model = H2ORandomForestEstimator(ntrees=10, max_depth=15, nfolds=10, binomial_double_trees=True, stopping_metric= \"auc\")\n",
    "        model.train(x=train_col, y=resp_col, training_frame=df_train)\n",
    "        pred=model.predict(df_val)\n",
    "        pr=h2o.h2o.as_list(pred, use_pandas=True)\n",
    "        tr=h2o.h2o.as_list(df_val, use_pandas=True)\n",
    "        false_positive_rate, true_positive_rate, thresholds = roc_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "        roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "        ROC_AUC.append(roc_auc)\n",
    "        precision, recall, thresholds=precision_recall_curve(tr.TX_FRAUD,pr.predict,pos_label=1)\n",
    "        average_precision=average_precision_score(tr.TX_FRAUD, pr.predict) \n",
    "        Average_Precision.append(average_precision)\n",
    "        n=[tr.CARD_PAN_ID,pr.predict,tr.TX_FRAUD]\n",
    "        new=pd.concat(n,axis=1)\n",
    "        gr=new.groupby(\"CARD_PAN_ID\").max().sort_values(\"predict\",ascending=False)\n",
    "        top=gr.head(100)\n",
    "        per=(top[top.TX_FRAUD==1].count())/100\n",
    "        perc=per.TX_FRAUD # frauds in top 100\n",
    "        Share.append(perc)\n",
    "        frauds=top[top.TX_FRAUD==1].index.tolist()\n",
    "        Fraud_ID=Fraud_ID+list(top[top.TX_FRAUD==1].index.unique())\n",
    "    print('The {} is done!'.format(file))\n",
    "    Final_table=pd.DataFrame({\"ROC_AUC\": ROC_AUC,\n",
    "                              \"Average_Precision\": Average_Precision,\n",
    "                              \"Share\": Share})\n",
    "A_P_m=Final_table.Average_Precision.mean()\n",
    "AUC_m=Final_table.ROC_AUC.mean()\n",
    "share_m=Final_table.Share.mean()\n",
    "print(\"Average precision mean: {}, ROC AUC mean: {}, Share of fraudulant card in top 100 mean: {}\".format(A_P_m, AUC_m, share_m))\n",
    "Final_table\n",
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_type_base=['object','object','int','object','float','object','object','float','object','object','object','object','int','float','int','int'\n",
    "         'object','object','object','object','float','float','float','object','object','int','int']\n",
    "col_name_base_raw=['CARD_PAN_ID','TERM_MIDUID','TERM_MCC','TERM_COUNTRY','TX_AMOUNT','TX_DATETIME','TX_3D_SECURE','AGE','LANGUAGE',\n",
    "                  'GENDER','BROKER','CARD_BRAND','TX_FRAUD','TX_TIME_SECONDS','TX_TIME_HOURS','TX_TIME_DAYS','TERM_REGION','TERM_CONTINENT',\n",
    "                  'TERM_MCCG','TERM_MCC_GROUP','MIN_AMT_LAST_24H','SUM_AMT_LAST_24H','TX_DIFF_LAST_TX','LAST_MIDUID_TX','LAST_COUNTRY_TX','LAST_MCC_HIS','NB_TRX_LAST_24H']\n",
    "col_name_base=col_name_base_raw[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#wazne!\n",
    "col_to_trans=['CARD_PAN_ID','TERM_MIDUID','TERM_MCC','TERM_COUNTRY','TX_3D_SECURE','LANGUAGE',\n",
    "                  'GENDER','BROKER','CARD_BRAND','TX_TIME_HOURS','TX_TIME_DAYS','TERM_REGION','TERM_CONTINENT',\n",
    "                  'TERM_MCCG','TERM_MCC_GROUP','TX_DIFF_LAST_TX','LAST_MIDUID_TX','LAST_COUNTRY_TX','LAST_MCC_HIS']\n",
    "for cols in col_to_trans:\n",
    "    b=df_s.groupby(cols).sum()/df_s.TX_FRAUD.sum()\n",
    "    df_s[cols]=df_s[cols].map(b.TX_FRAUD.to_dict())\n",
    "df_s.TX_DATETIME=((pd.to_datetime(df_s.TX_DATETIME).dt.month)*100+pd.to_datetime(df_s.TX_DATETIME).dt.day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trying to learn something new\n",
    "raw_data = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'],\n",
    "        'company': ['1st', '1st', '2nd', '2nd', '1st', '1st', '2nd', '2nd','1st', '1st', '2nd', '2nd'],\n",
    "        'name': ['Miller', 'Jacobson', 'Ali', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Sone', 'Sloan', 'Piger', 'Riani', 'Ali'],\n",
    "        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70],\n",
    "        'fraud': [0,0,0,1,0,1,0,1,0,0,0,1]}\n",
    "df = pd.DataFrame(raw_data, columns = ['regiment', 'company', 'name', 'preTestScore', 'postTestScore','fraud'])\n",
    "dft=df.copy(deep=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b=df.groupby('company').sum()/df.fraud.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_to_trans=['regiment','company','name']\n",
    "\n",
    "for cols in col_to_trans:\n",
    "    b=df.groupby(cols).sum()/df.fraud.sum()\n",
    "    #specname = cols.split(\"/\")[-1]\n",
    "    df[cols]=df[cols].map(b.fraud.to_dict())\n",
    "\n",
    "\n",
    "del df[\"preTestScore\"]\n",
    "del df[\"postTestScore\"]\n",
    "del df[\"fraud\"]\n",
    "df.columns=['risk_regiment','risk_company','risk_name']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_data1 = {'regiment': ['Nighthawks', 'Nighthawks', 'Nighthawks', 'Nighthawks', 'Dragoons', 'Dragoons', 'Dragoons', 'Dragoons', 'Scouts', 'Scouts', 'Scouts', 'Scouts'],\n",
    "        'company': ['1st', '1st', '2nd', '3nd', '2st', '1st', '2nd', '1nd','1st', '3st', '2nd', '2nd'],\n",
    "        'name': ['Miller', 'Jacobson', 'May', 'Milner', 'Cooze', 'Jacon', 'Ryaner', 'Ali', 'Sloan', 'Piger', 'Ben', 'Ali'],\n",
    "        'preTestScore': [4, 24, 31, 2, 3, 4, 24, 31, 2, 3, 2, 3],\n",
    "        'postTestScore': [25, 94, 57, 62, 70, 25, 94, 57, 62, 70, 62, 70],\n",
    "        'fraud': [0,0,1,1,0,1,0,1,0,1,0,1]}\n",
    "df1 = pd.DataFrame(raw_data1, columns = ['regiment', 'company', 'name', 'preTestScore', 'postTestScore','fraud'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new=pd.merge(dft, df1, how='inner', on=['name'])\n",
    "lis=new['name']\n",
    "dft.name[dft.name==\"Ali\"]\n",
    "#df.loc[[2]]['name']\n",
    "#df1['name'][df1.name==\"Ali\"]==0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dft.name[new['name']].index\n",
    "#dft.info()   \n",
    "d=pd.concat([dft.regiment,dft.company,dft.name,dft.preTestScore,dft.postTestScore,df.risk_regiment,df.risk_company,df.risk_name], axis=1)      \n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def factor_from_previous_risk(df, df_past,feature_to_transform):\n",
    "    for factor in feature_to_transform:\n",
    "            riskfactor = \"risk_\" + factor\n",
    "            #dffact = df[[factor]]\n",
    "            factor_risk = df_past[[factor, riskfactor]]\n",
    "            factor_risk=factor_risk.drop_duplicates(subset=factor, keep='last')\n",
    "            df = df.merge(factor_risk, how='left', on=factor)\n",
    "            nbrna = df[riskfactor].isnull().sum()\n",
    "            nbrrow = df.shape[0]\n",
    "            print(\"No risk found for factor {}  in {}% of cases\".format(riskfactor, (nbrna/nbrrow)*100 ))\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "factor_from_previous_risk(df1,d,col_to_trans)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
