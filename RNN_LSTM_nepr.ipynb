{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from sklearn import metrics\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1=pd.read_csv(\"Temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=np.array(d1.V1)\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_periods=200 # size of the batch\n",
    "f_horizon=1\n",
    "inputs=1\n",
    "output=1\n",
    "Data=data[:(len(data)-(len(data) % num_periods))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visualisation():\n",
    "    plt.plot(y_test[0],label='Actual')\n",
    "    plt.plot(y_pred[0],label='Forecast')\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    return plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_prep(data,num_periods=200,f_horizon=1,inputs=1,output=1):\n",
    "    data_train=data[0:1210]\n",
    "    x_train=data_train[:(len(data_train)-(len(data_train)%num_periods))]\n",
    "    y_train=data_train[f_horizon:(len(data_train)-(len(data_train)%num_periods)+f_horizon)]\n",
    "    x_batches=x_train.reshape(-1,num_periods,inputs)\n",
    "    y_batches=y_train.reshape(-1,num_periods,output)\n",
    "    data_test=data[1210:len(data)]\n",
    "    x_test=data_test[:(len(data_test)-(len(data_test)%num_periods))]\n",
    "    y_test=data_test[f_horizon:(len(data_test)-(len(data_test)%num_periods)+f_horizon)]\n",
    "    x_test=x_test.reshape(-1,num_periods,inputs)\n",
    "    y_test=y_test.reshape(-1,num_periods,output)\n",
    "    return x_batches, y_batches, x_test, y_test\n",
    "\n",
    "x_batches, y_batches, x_test, y_test=data_prep(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches, y_batches, x_test, y_test=data_prep(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(x_batches))\n",
    "print(len(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(x_test)\n",
    "np.shape(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=1\n",
    "hidden=100\n",
    "output=1\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE=(mse/len(x_train))/np.var(x_train)\n",
    "    #NMSE_test=(mse_test/len(x_test.reshape(-1)))/np.var(x_test.reshape(-1))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num_per=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches, y_batches, x_test, y_test=data_prep(data,num_periods=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=50\n",
    "inputs=1\n",
    "hidden=100\n",
    "output=1\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE=(mse/len(x_train))/np.var(x_train)\n",
    "    #NMSE_test=(mse_test/len(x_test.reshape(-1)))/np.var(x_test.reshape(-1))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Num_per=200, lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batches, y_batches, x_test, y_test=data_prep(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=1\n",
    "hidden=100\n",
    "output=1\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden)#,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE=(mse/len(x_train))/np.var(x_train)\n",
    "    #NMSE_test=(mse_test/len(x_test.reshape(-1)))/np.var(x_test.reshape(-1))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of 2 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches, y_batches, x_test, y_test=data_prep(data,f_horizon=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=1\n",
    "hidden=100\n",
    "output=1\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "#basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden,activation=tf.nn.relu)\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    # NMSE=(mse/len(x_train))/np.var(x_train)\n",
    "    #NMSE_test=(mse_test/len(x_test.reshape(-1)))/np.var(x_test.reshape(-1))\n",
    "    #NMSE_test=(mse_test/len(x_test.reshape(-1)))/(np.sum(np.square(y_pred-np.mean(x_train)))/len(x_test.reshape(-1)))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test[0],y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test[0],y_pred[0])/(np.sum(np.square(y_pred-np.mean(x_train)))/len(x_test.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred.reshape(-1)[0:20]\n",
    "y_test.reshape(-1)[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Forecast vs Actual\", fontsize=14)\n",
    "plt.plot(pd.Series(np.ravel(y_test.reshape(-1)[0:20])), \"bo\", markersize=10, label=\"Actual\")\n",
    "#plt.plot(pd.Series(np.ravel(Y_test)), \"w*\", markersize=10)\n",
    "plt.plot(pd.Series(np.ravel(y_pred.reshape(-1)[0:20])), \"r.\", markersize=10, label=\"Forecast\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel(\"Time Periods\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two features learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=np.array([d1.V1,d1.V2])\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(data[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Original: num_periods=200,f_horizon=1, hodden=100 lr=0.001\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def data_prep_n(data,num_periods=200,f_horizon=1):#,tr_unit=1200):\n",
    "    Data=[]\n",
    "    for i in range(len(data[:,0])):\n",
    "        Data.append(data[i])#[:(len(data[i])-(len(data[i]) % num_periods))])\n",
    "    Data=np.array(Data)\n",
    "    data_train=Data[:,0:1200]\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_test=[]\n",
    "    y_test=[]\n",
    "    data_test=Data[:,1200:len(Data[0])]\n",
    "    for j in range(len(data[:,0])):\n",
    "        x_train.append(data_train[j][:(len(data_train[j])-(len(data_train[j])%num_periods))])\n",
    "        y_train.append(data_train[j][f_horizon:(len(data_train[j])-(len(data_train[j])%num_periods)+f_horizon)])\n",
    "        x_test.append(data_test[j][:(len(data_test[j])-(len(data_test[j])%num_periods))])\n",
    "        y_test.append(data_test[j][f_horizon:(len(data_test[j])-(len(data_test[j])%num_periods)+f_horizon)])\n",
    "    x_train=np.array(x_train)\n",
    "    y_train=np.array(y_train) \n",
    "    x_test=np.array(x_test)\n",
    "    y_test=np.array(y_test)\n",
    "    print(len(x_train[0]))\n",
    "    print(len(y_train[0]))\n",
    "    x_batches=(np.stack(x_train,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "    y_batches=(np.stack(y_train,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "    x_test=(np.stack(x_test,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "    y_test=(np.stack(y_test,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "    return x_batches, y_batches, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches, y_batches, x_test, y_test=data_prep_n(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_periods=200 # size of the batch\n",
    "f_horizon=1\n",
    "Data=np.array([data[0][:(len(data[0])-(len(data[0]) % num_periods))],data[1][:(len(data[1])-(len(data[1]) % num_periods))]])\n",
    "data_train=Data[:,0:1210]\n",
    "x_train=np.array([data_train[0][:(len(data_train[0])-(len(data_train[0])%num_periods))],data_train[1][:(len(data_train[1])-(len(data_train[1])%num_periods))]])\n",
    "y_train=np.array([data_train[0][f_horizon:(len(data_train[0])-(len(data_train[0])%num_periods)+f_horizon)],data_train[1][f_horizon:(len(data_train[1])-(len(data_train[1])%num_periods)+f_horizon)]])\n",
    "x_batches=(np.stack((x_train[0],x_train[1]),axis=-1)).reshape(-1,num_periods,2)\n",
    "y_batches=(np.stack((y_train[0],y_train[1]),axis=-1)).reshape(-1,num_periods,2)\n",
    "data_test=Data[:,1210:len(Data[0])]\n",
    "x_test=np.array([data_test[0][:(len(data_test[0])-(len(data_test[0])%num_periods))],data_test[1][:(len(data_test[1])-(len(data_test[1])%num_periods))]])\n",
    "y_test=np.array([data_test[0][f_horizon:(len(data_test[0])-(len(data_test[0])%num_periods)+f_horizon)],data_test[1][f_horizon:(len(data_test[1])-(len(data_test[1])%num_periods)+f_horizon)]])\n",
    "x_test=(np.stack((x_test[0],x_test[1]),axis=-1)).reshape(-1,num_periods,2)\n",
    "y_test=(np.stack((y_test[0],y_test[1]),axis=-1)).reshape(-1,num_periods,2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=2\n",
    "hidden=100\n",
    "output=2\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "#loss=tf.reduce_mean(tf.matmul((outputs-Y),tf.transpose((outputs-Y), perm=[0,2,1])))\n",
    "#loss=tf.reduce_mean(tf.reduce_sum(tf.square(outputs-Y),axis=0))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE=(mse/len(x_train[0]))/np.var(x_train)\n",
    "    #NMSE_test=(mse_test/len(x_test[0]))/np.var(x_test.reshape(-1))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "    #print(NMSE)\n",
    "    #print(NMSE_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.shape(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,1])\n",
    "plt.plot(y_pred[0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,0])\n",
    "plt.plot(y_pred[0][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test[0][:,1],y_pred[0][:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_periods=200,f_horizon=1, hodden=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=2\n",
    "hidden=200\n",
    "output=2\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "#loss=tf.reduce_sum(tf.matmul((outputs-Y),tf.transpose((outputs-Y), perm=[0,2,1])))\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "#loss=tf.reduce_mean(tf.reduce_sum(tf.square(outputs-Y),axis=0))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE_test=metrics.mean_squared_error(y_test[0],y_pred[0])/(np.mean(np.sum(np.subtract(y_pred,np.mean(x_train,axis=1)),axis=1)/len(y_test[0])))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "    #print(NMSE)\n",
    "    #print(NMSE_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,1])\n",
    "plt.plot(y_pred[0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,0])\n",
    "plt.plot(y_pred[0][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.mean_squared_error(y_test[0],y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean((np.sum(np.square(y_test[0]-y_pred[0]),axis=0))/len(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.mean(np.sum(np.subtract(y_test[0],np.mean(x_train,axis=1)),axis=0)/len(y_test[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NMSE_1=metrics.mean_squared_error(y_test[0],y_pred[0])/(np.mean(y_pred)*np.mean(y_test)) #???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NMSE=(np.mean((np.sum(np.square(y_test[0]-y_pred[0]),axis=0))/len(y_test[0])))/(np.mean(np.sum(np.subtract(y_pred,np.mean(x_train,axis=1)),axis=1)/len(y_test[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Data preparation\n",
    "Data=[]\n",
    "for i in range(len(data[:,0])):\n",
    "    Data.append(data[i][:(len(data[i])-(len(data[i]) % num_periods))])\n",
    "Data=np.array(Data)\n",
    "data_train=Data[:,0:1210]\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "x_test=[]\n",
    "y_test=[]\n",
    "data_test=Data[:,1210:len(Data[0])]\n",
    "for j in range(len(data[:,0])):\n",
    "    x_train.append(data_train[j][:(len(data_train[j])-(len(data_train[j])%num_periods))])\n",
    "    y_train.append(data_train[j][f_horizon:(len(data_train[j])-(len(data_train[j])%num_periods)+f_horizon)])\n",
    "    x_test.append(data_test[j][:(len(data_test[j])-(len(data_test[j])%num_periods))])\n",
    "    y_test.append(data_test[j][f_horizon:(len(data_test[j])-(len(data_test[j])%num_periods)+f_horizon)])\n",
    "x_train=np.array(x_train)\n",
    "y_train=np.array(y_train) \n",
    "x_test=np.array(x_test)\n",
    "y_test=np.array(y_test)\n",
    "x_batches=(np.stack(x_train,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "y_batches=(np.stack(y_train,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "x_test=(np.stack(x_test,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "y_test=(np.stack(y_test,axis=-1)).reshape(-1,num_periods,len(data[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=np.array([d1.V1,d1.V2,d1.V3,d1.V4])\n",
    "np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=len(data[:,0])\n",
    "hidden=200\n",
    "output=len(data[:,0])\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden)#,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "#loss=tf.reduce_sum(tf.matmul((outputs-Y),tf.transpose((outputs-Y), perm=[0,2,1])))\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "#loss=tf.reduce_mean(tf.reduce_sum(tf.square(outputs-Y),axis=0))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE_test=metrics.mean_squared_error(y_test[0],y_pred[0])/(np.mean(np.sum(np.subtract(y_pred,np.mean(x_train,axis=1)),axis=1)/len(y_test[0])))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "    #print(NMSE)\n",
    "    #print(NMSE_test)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,0])\n",
    "plt.plot(y_pred[0][:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,1])\n",
    "plt.plot(y_pred[0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,2])\n",
    "plt.plot(y_pred[0][:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0][:,3])\n",
    "plt.plot(y_pred[0][:,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # New composition of training and test sets\n",
    " (predict the values of one column given the values of the other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_v=np.array(d1.V10)\n",
    "np.shape(data_v)\n",
    "data_l=np.array(d1.V11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_periods=200 # size of the batch\n",
    "f_horizon=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#values\n",
    "Data_v=data_v[:(len(data_v)-(len(data_v) % num_periods))]\n",
    "data_train_v=Data_v[0:1210]\n",
    "x_train=data_train_v[:(len(data_train_v)-(len(data_train_v)%num_periods))]\n",
    "x_batches=x_train.reshape(-1,num_periods,1)\n",
    "data_test_v=Data_v[1210:len(Data_v)]\n",
    "x_test=data_test_v[:(len(data_test_v)-(len(data_test_v)%num_periods))]\n",
    "x_test=x_test.reshape(-1,num_periods,1)\n",
    "#labels\n",
    "Data_l=data_l[:(len(data_l)-(len(data_l) % num_periods))]\n",
    "data_train_l=Data_l[0:1210]\n",
    "y_train=data_train_l[:(len(data_train_l)-(len(data_train_l)%num_periods))]\n",
    "y_batches=y_train.reshape(-1,num_periods,1)\n",
    "data_test_l=Data_l[1210:len(Data_l)]\n",
    "y_test=data_test_l[:(len(data_test_l)-(len(data_test_l)%num_periods))]\n",
    "y_test=y_test.reshape(-1,num_periods,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=1\n",
    "hidden=100\n",
    "output=1\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden)#,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE=(mse/len(x_train))/np.var(x_train)\n",
    "    # NMSE_test=(mse_test/len(x_test.reshape(-1)))/np.var(x_test.reshape(-1))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0],label='Actula')\n",
    "plt.plot(y_pred[0],label='Forecast')\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One step ahead prediction of the on feature based on the values of the other feature \n",
    "(same as previous but one step ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#values\n",
    "Data_v=data_v[:(len(data_v)-(len(data_v) % num_periods))]\n",
    "data_train_v=Data_v[0:1210]\n",
    "x_train=data_train_v[:(len(data_train_v)-(len(data_train_v)%num_periods))]\n",
    "x_batches=x_train.reshape(-1,num_periods,1)\n",
    "data_test_v=Data_v[1210:len(Data_v)]\n",
    "x_test=data_test_v[:(len(data_test_v)-(len(data_test_v)%num_periods))]\n",
    "x_test=x_test.reshape(-1,num_periods,1)\n",
    "#labels\n",
    "Data_l=data_l[:(len(data_l)-(len(data_l) % num_periods))]\n",
    "data_train_l=Data_l[0:1210]\n",
    "y_train=data_train_l[1:(len(data_train_l)-(len(data_train_l)%num_periods)+1)]\n",
    "y_batches=y_train.reshape(-1,num_periods,1)\n",
    "data_test_l=Data_l[1210:len(Data_l)]\n",
    "y_test=data_test_l[1:(len(data_test_l)-(len(data_test_l)%num_periods)+1)]\n",
    "y_test=y_test.reshape(-1,num_periods,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "num_periods=200\n",
    "inputs=1\n",
    "hidden=150\n",
    "output=1\n",
    "X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden)#,activation=tf.nn.relu)\n",
    "rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "learning_rat=0.001\n",
    "stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "training_op=optimizer.minimize(loss)\n",
    "init=tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs=2000\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for ep in range(epochs):\n",
    "        sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "        if ep % 100 == 0:\n",
    "            mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "            print(ep,\"\\tMSE:\",mse)\n",
    "    y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "    mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #NMSE=(mse/len(x_train))/np.var(x_train)\n",
    "    # NMSE_test=(mse_test/len(x_test.reshape(-1)))/np.var(x_test.reshape(-1))\n",
    "    print(mse)\n",
    "    print(mse_test)\n",
    "sess.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test[0],label='Actula')\n",
    "plt.plot(y_pred[0],label='Forecast')\n",
    "plt.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# One feature one step ahead prediction based of the values of several features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=np.array([d1.V1,d1.V2,d1.V3,d1.V4,d1.V5,d1.V6,d1.V7,d1.V8,d1.V9,d1.V10])\n",
    "data_l=np.array(d1.V11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dat_val_prep(data,num_periods=200,f_horizon=1,num_unit=1210):\n",
    "    Data=[]\n",
    "    for i in range(len(data[:,0])):\n",
    "        Data.append(data[i][:(len(data[i])-(len(data[i]) % num_periods))])\n",
    "    Data=np.array(Data)\n",
    "    data_train=Data[:,0:num_unit]\n",
    "    x_train=[]\n",
    "    x_test=[]\n",
    "    data_test=Data[:,num_unit:len(Data[0])]\n",
    "    for j in range(len(data[:,0])):\n",
    "        x_train.append(data_train[j][:(len(data_train[j])-(len(data_train[j])%num_periods))])\n",
    "        x_test.append(data_test[j][:(len(data_test[j])-(len(data_test[j])%num_periods))])\n",
    "    x_train=np.array(x_train) \n",
    "    x_test=np.array(x_test)\n",
    "    x_batches=(np.stack(x_train,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "    x_test=(np.stack(x_test,axis=-1)).reshape(-1,num_periods,len(data[:,0]))\n",
    "    x_test[np.isnan(x_test)] = 0\n",
    "    x_batches[np.isnan(x_batches)]=0\n",
    "    return x_batches, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches, x_test=dat_val_prep(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dat_labl_prep(data_l,num_periods=200,f_horizon=1,num_unit=1210):  \n",
    "    Data_l=data_l[:(len(data_l)-(len(data_l) % num_periods))]\n",
    "    data_train_l=Data_l[0:num_unit]\n",
    "    y_train=data_train_l[1:(len(data_train_l)-(len(data_train_l)%num_periods)+1)]\n",
    "    y_batches=y_train.reshape(-1,num_periods,1)\n",
    "    data_test_l=Data_l[num_unit:len(Data_l)]\n",
    "    y_test=data_test_l[1:(len(data_test_l)-(len(data_test_l)%num_periods)+1)]\n",
    "    y_test=y_test.reshape(-1,num_periods,1)\n",
    "    y_batches[np.isnan(y_batches)]=0\n",
    "    y_test[np.isnan(y_test)]=0\n",
    "    return y_batches, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_batches,y_test=dat_labl_prep(data_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(x_batches),np.shape(y_batches),np.shape(x_test),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph_RNN(data,epochs,num_periods=200,hidden=200,output=1,learning_rat=0.00065):\n",
    "    tf.reset_default_graph()\n",
    "    inputs=len(data[:,0])\n",
    "    X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "    Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "    basic_cell=tf.contrib.rnn.BasicRNNCell(num_units=hidden)\n",
    "    rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "    stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "    stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "    outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "    loss=tf.reduce_mean(tf.squared_difference(outputs, Y))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for ep in range(epochs):\n",
    "            sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "            if ep % 100 == 0:\n",
    "                mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "                print(ep,\"\\tMSE:\",mse)\n",
    "        y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "        mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "    #print(mse)\n",
    "    #print(mse_test)\n",
    "    print(mse, mse_test)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=build_graph_RNN(data,2000,hidden=150,learning_rat=0.01)\n",
    "#0.0272222 3.68108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=build_graph_RNN(data,1300,hidden=150,learning_rat=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualisation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM (1 feature-1feature) Data:Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=np.array(d1.V1)\n",
    "np.shape(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_batches, y_batches, x_test, y_test=data_prep(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(np.shape(x_batches),np.shape(y_batches),np.shape(x_test),np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph_LSTM(data,epochs,num_periods=200,inputs=1,hidden=200,output=1,learning_rat=0.001):\n",
    "    tf.reset_default_graph()\n",
    "    X=tf.placeholder(tf.float32,[None,num_periods,inputs])\n",
    "    Y=tf.placeholder(tf.float32,[None,num_periods,output])\n",
    "    basic_cell=tf.contrib.rnn.BasicLSTMCell(num_units=hidden,activation=tf.tanh)\n",
    "    rnn_output, states=tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)\n",
    "    stacked_rnn_output=tf.reshape(rnn_output,[-1,hidden])\n",
    "    stacked_outputs=tf.layers.dense(stacked_rnn_output,output)\n",
    "    outputs=tf.reshape(stacked_outputs,[-1,num_periods,output])\n",
    "    loss=(tf.reduce_mean(tf.squared_difference(outputs, Y)))\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rat)\n",
    "    training_op=optimizer.minimize(loss)\n",
    "    init=tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        for ep in range(epochs):\n",
    "            sess.run(training_op,feed_dict={X: x_batches, Y: y_batches})\n",
    "            if ep % 100 == 0:\n",
    "                mse=loss.eval(feed_dict={X: x_batches, Y: y_batches})\n",
    "                print(ep,\"\\tMSE:\",mse)\n",
    "        y_pred=sess.run(outputs,feed_dict={X:x_test})\n",
    "        mse_test=loss.eval(feed_dict={X:x_test,Y:y_test})\n",
    "   \n",
    "        print(mse, mse_test)\n",
    "    \n",
    "    sess.close()\n",
    "    return y_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=build_graph_LSTM(data,1000,hidden=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualisation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Many to one feature one step ahead. (To compare RNN and LSTM performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_batches, x_test=dat_val_prep(data)\n",
    "y_batches,y_test=dat_labl_prep(data_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred=build_graph_LSTM(data,200,hidden=150,inputs=len(data[:,0]),learning_rat=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "visualisation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t=np.arange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
